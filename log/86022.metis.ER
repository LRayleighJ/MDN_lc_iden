Traceback (most recent call last):
  File "/home/zerui603/MDN_lc_iden/unet/unet_kmt.py", line 937, in <module>
    training(paramsid=0)
  File "/home/zerui603/MDN_lc_iden/unet/unet_kmt.py", line 127, in training
    outputs = network(inputs)
  File "/home/zerui603/.conda/envs/PytorchCd11/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zerui603/.conda/envs/PytorchCd11/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/zerui603/.conda/envs/PytorchCd11/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/zerui603/.conda/envs/PytorchCd11/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/zerui603/.conda/envs/PytorchCd11/lib/python3.7/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/zerui603/.conda/envs/PytorchCd11/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/zerui603/.conda/envs/PytorchCd11/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zerui603/MDN_lc_iden/unet/netmodule/unetforkmt.py", line 170, in forward
    x,_ = self.rnn(x,None)
  File "/home/zerui603/.conda/envs/PytorchCd11/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zerui603/.conda/envs/PytorchCd11/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 850, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 574.00 MiB (GPU 0; 31.75 GiB total capacity; 417.99 MiB already allocated; 558.69 MiB free; 436.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

